Pedicted:
-actual

x = input

input to output

1/m SUM (x(dot)theta - y)^2

or replace x with h(x) so a function instead

x(dot)theta is the line and y is the outcome
The equation finds the distance from the answer to the line
which is considered the error

less good fit the farther away it is

SVM find a line such that one class is on one side and another ont he other

y = mx + b

we need to generalize the equation as

y = w_0 + w_1x_1 + w_2x_2 ...
(petal width is 1 and petal length is 2)
n dimensional plain in an n dimensional space

mx + b - y = 0

Using gradients is to find the minimum

3 types of
stochastic regression (single instance)
batch regression
mini batch regression

Perceptron:

\
- [p] - outputs 0 or 1 (one model)
/

w_1x_1 + w_2x_2 ... >  b => 1
                    <= b => 0

b is a threshold

gradient decent doesn't work well with logistic outcome (0 or 1)

Sigmoid

1/(1 + e^-x)

SUM(sigma(X(dot)w - y)^2)/m

w_0 = 1.5

w_0 + x_1 + x_2 = y
w_0 + w_1*x_1 + w_2*x_2 = y

                 y
x_1 | x_2 || x_1 & x_2 | w_0 | w_1 | w_2 | w_1+2+3 | w_i + error |
-----------------------------------------------------------------
 0  |  0  ||     0     |-1.5 |  1  |  1  |  -1.5   |
 0  |  1  ||     0     |-1.5 |  1  |  1  |   -.5   |
 1  |  0  ||     0     |-1.5 |  1  |  1  |   -.5   |
 1  |  1  ||     1     |-1.5 |  1  |  1  |    .5   |


